import torch

'''
ELMo
    : 같은 표기의 단어라도 문맥에 따라 다르게 해석되는 경우가 있다. 하지만 이는 통상적인 word embedding에서는 나타내기 어렵다. 
    그래서 Contextualized Word Embedding 이라는 아이디어가 나옴
    
    특징
        1. Bidirectional Language Model : 순방향 RNN과 역방향 RNN을 모두 사용, 다만 양방향 RNN의 경우 순 방향의 은닉 상태와 역 방향의 은닉 상태
        를 concat 해서 사용하는 반면, biLM은 두 개의 언어 모델을 별 개의 모델로 보고 학습
        2. 은닉 층이 최소 2개 이상

    biLM의 활용
        1. 학습하고자 하는 각 위치의 있는 모든 층의 출력 값을 순방향 모델과 역방향 모델에서 각각 가져와 층끼리 concat 한다
        2. 각 층의 출력값 별로 가중치를 둔다
        3. 모두 더한다
        4. 벡터의 크기를 결정하는 스칼라 매개 변수를 곱한다.
        
이러한 ELMo 표현은 기존 임베딩 벡터와 함께 사용할 수 있다. 하나의 단어에 대해 두 모델에서 생성된 벡터 값을 이어 붙여(concatenate) 사용하면 된다.
'''
