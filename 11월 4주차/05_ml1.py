# -*- coding: utf-8 -*-
"""05_ML1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OIz__MlomIp1YniGiEwpOHkBGZFd1FG2
"""

import torch

USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴
device = torch.device("cuda" if USE_CUDA else "cpu") # GPU 사용 가능하면 사용하고 아니면 CPU 사용
print("다음 기기로 학습합니다:", device)

from re import M
import torch
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import torch.nn as nn
import matplotlib.pyplot as plt
import random
from tqdm import tqdm
import torch.nn.functional as F

random.seed(777)
torch.manual_seed(777)
if device == "cuda":
    torch.cuda.manual_seed_all(777)

training_epoch = 200
batch_size = 100

transform_flatten = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.flatten())
])

mnist_train = dsets.MNIST(root='MNIST_data/',
                          train=True,
                          transform=transform_flatten,
                          download=True)

mnist_test = dsets.MNIST(root='MNIST_data/',
                         train=False,
                         transform=transforms.ToTensor(),
                         download=True)


train_data = DataLoader(dataset=mnist_train,
                        batch_size=batch_size,
                        shuffle=True,
                        drop_last=True,     # 마지막 불완전한 배치를 생략     # 워커 수 설정
                        pin_memory=True)


class myModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layer1 = nn.Sequential(nn.Linear(784, 100, bias=True),
                                    nn.ReLU(),
                                    nn.Linear(100,10, bias=True))
    def forward(self, x):
        out = self.layer1(x)
        return out

model = myModel().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)


for epoch in range(training_epoch):
    avg_loss = 0
    total_batch = len(train_data)
    with tqdm(total=len(train_data), desc=f"Epoch {epoch}/{training_epoch}") as pbar:
        for idx, data in enumerate(train_data):
            x_train, y_train = data
            x_train = x_train.to(device)
            y_train = y_train.to(device)

            prediction = model(x_train).squeeze()

            loss = F.cross_entropy(prediction, y_train)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            avg_loss += loss / total_batch

            pbar.update(1)

    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)
    Y_test = mnist_test.test_labels.to(device)

    prediction = model(X_test)
    correct = torch.argmax(prediction, dim=1) == Y_test
    accuracy = correct.float().mean() * 100

    print(f'Epoch: {epoch} / {training_epoch}, Loss: {avg_loss.item():.4f}, accuracy : {accuracy.item():.2f}%')















